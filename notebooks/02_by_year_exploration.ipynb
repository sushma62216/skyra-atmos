{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA GHCN By-Year Data Exploration\n",
    "\n",
    "This notebook explores the daily climate observations from the NOAA GHCN dataset, organized by year in Parquet format.\n",
    "\n",
    "**Data Source:** `s3://noaa-ghcn-pds/parquet/by_year/`\n",
    "\n",
    "**Structure:** Partitioned by year (e.g., `YEAR=2023/`)\n",
    "\n",
    "**Key Fields:**\n",
    "- `ID` - Station identifier\n",
    "- `DATE` - Observation date (YYYYMMDD)\n",
    "- `ELEMENT` - Type of observation (TMAX, TMIN, PRCP, etc.)\n",
    "- `DATA_VALUE` - The observation value\n",
    "- Quality/source flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install pandas pyarrow s3fs fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "# S3 filesystem (no credentials needed for public bucket)\n",
    "S3_BUCKET = 'noaa-ghcn-pds'\n",
    "PARQUET_PATH = f's3://{S3_BUCKET}/parquet/by_year'\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Available Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available year partitions\n",
    "year_partitions = fs.ls(f'{S3_BUCKET}/parquet/by_year/')\n",
    "years = sorted([int(p.split('YEAR=')[1].rstrip('/')) for p in year_partitions if 'YEAR=' in p])\n",
    "\n",
    "print(f\"Available years: {min(years)} to {max(years)}\")\n",
    "print(f\"Total years of data: {len(years)}\")\n",
    "print(f\"\\nFirst 10 years: {years[:10]}\")\n",
    "print(f\"Last 10 years: {years[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Schema and Sample Data\n",
    "\n",
    "Let's look at a recent year to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read schema from a sample file\n",
    "sample_year = 2023\n",
    "sample_path = f'{S3_BUCKET}/parquet/by_year/YEAR={sample_year}'\n",
    "\n",
    "# Get parquet files in this partition\n",
    "parquet_files = fs.ls(sample_path)\n",
    "print(f\"Files in YEAR={sample_year}: {len(parquet_files)}\")\n",
    "print(f\"\\nFirst file: {parquet_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read schema\n",
    "with fs.open(parquet_files[0], 'rb') as f:\n",
    "    schema = pq.read_schema(f)\n",
    "\n",
    "print(\"Parquet Schema:\")\n",
    "print(\"=\" * 40)\n",
    "for field in schema:\n",
    "    print(f\"  {field.name}: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample data from one year\n",
    "df_sample = pd.read_parquet(\n",
    "    f's3://{sample_path}',\n",
    "    storage_options={'anon': True}\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(df_sample.dtypes)\n",
    "print(f\"\\nSample data:\")\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage\n",
    "print(f\"Memory usage for year {sample_year}:\")\n",
    "print(df_sample.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Element Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by element type\n",
    "element_counts = df_sample['ELEMENT'].value_counts()\n",
    "print(f\"Observation counts by element type ({sample_year}):\")\n",
    "print(element_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element descriptions\n",
    "element_descriptions = {\n",
    "    'PRCP': 'Precipitation (tenths of mm)',\n",
    "    'SNOW': 'Snowfall (mm)',\n",
    "    'SNWD': 'Snow depth (mm)',\n",
    "    'TMAX': 'Maximum temperature (tenths of degrees C)',\n",
    "    'TMIN': 'Minimum temperature (tenths of degrees C)',\n",
    "    'TAVG': 'Average temperature (tenths of degrees C)',\n",
    "    'AWND': 'Average daily wind speed (tenths of m/s)',\n",
    "    'WSFG': 'Peak gust wind speed (tenths of m/s)',\n",
    "    'WT01': 'Fog, ice fog, or freezing fog',\n",
    "    'WT02': 'Heavy fog or heavy freezing fog',\n",
    "    'WT03': 'Thunder',\n",
    "    'WT04': 'Ice pellets, sleet, snow pellets',\n",
    "    'WT05': 'Hail',\n",
    "    'WT06': 'Glaze or rime',\n",
    "    'WT07': 'Dust, volcanic ash, sand, or spray',\n",
    "    'WT08': 'Smoke or haze'\n",
    "}\n",
    "\n",
    "print(\"Common element codes and meanings:\")\n",
    "for code, desc in element_descriptions.items():\n",
    "    print(f\"  {code}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Flags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_FLAG - Measurement flag\n",
    "print(\"M_FLAG (Measurement) distribution:\")\n",
    "print(df_sample['M_FLAG'].value_counts(dropna=False).head(10))\n",
    "\n",
    "print(\"\\nM_FLAG meanings:\")\n",
    "print(\"  Blank = no flag\")\n",
    "print(\"  B = precipitation total from two 12-hour totals\")\n",
    "print(\"  D = precipitation total from four 6-hour totals\")\n",
    "print(\"  H = precipitation total from eight 3-hour totals\")\n",
    "print(\"  L = temperature lagged from previous day\")\n",
    "print(\"  T = trace of precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_FLAG - Quality flag\n",
    "print(\"Q_FLAG (Quality) distribution:\")\n",
    "print(df_sample['Q_FLAG'].value_counts(dropna=False).head(10))\n",
    "\n",
    "print(\"\\nQ_FLAG meanings:\")\n",
    "print(\"  Blank = passed all quality checks\")\n",
    "print(\"  D = failed duplicate check\")\n",
    "print(\"  G = failed gap check\")\n",
    "print(\"  I = failed internal consistency check\")\n",
    "print(\"  K = failed streak/frequent-value check\")\n",
    "print(\"  N = failed naught check\")\n",
    "print(\"  S = failed spatial consistency check\")\n",
    "print(\"  X = failed bounds check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_FLAG - Source flag\n",
    "print(\"S_FLAG (Source) distribution:\")\n",
    "print(df_sample['S_FLAG'].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Station Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations reporting in this year\n",
    "stations_in_year = df_sample['ID'].nunique()\n",
    "print(f\"Unique stations reporting in {sample_year}: {stations_in_year:,}\")\n",
    "\n",
    "# Stations by country (first 2 chars of ID)\n",
    "df_sample['country'] = df_sample['ID'].str[:2]\n",
    "stations_by_country = df_sample.groupby('country')['ID'].nunique().sort_values(ascending=False)\n",
    "print(f\"\\nTop 15 countries by station count:\")\n",
    "print(stations_by_country.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations per station\n",
    "obs_per_station = df_sample.groupby('ID').size()\n",
    "print(f\"Observations per station statistics:\")\n",
    "print(f\"  Min: {obs_per_station.min():,}\")\n",
    "print(f\"  Max: {obs_per_station.max():,}\")\n",
    "print(f\"  Mean: {obs_per_station.mean():,.0f}\")\n",
    "print(f\"  Median: {obs_per_station.median():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temperature Data Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to temperature data only\n",
    "temp_data = df_sample[df_sample['ELEMENT'].isin(['TMAX', 'TMIN', 'TAVG'])].copy()\n",
    "\n",
    "# Convert to actual Celsius (divide by 10)\n",
    "temp_data['temp_celsius'] = temp_data['DATA_VALUE'] / 10\n",
    "\n",
    "print(f\"Temperature observations: {len(temp_data):,}\")\n",
    "print(f\"\\nTemperature statistics (Celsius):\")\n",
    "temp_data.groupby('ELEMENT')['temp_celsius'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme temperatures\n",
    "print(\"Extreme TMAX values (hottest):\")\n",
    "hottest = temp_data[temp_data['ELEMENT'] == 'TMAX'].nlargest(10, 'temp_celsius')[['ID', 'DATE', 'temp_celsius']]\n",
    "print(hottest)\n",
    "\n",
    "print(\"\\nExtreme TMIN values (coldest):\")\n",
    "coldest = temp_data[temp_data['ELEMENT'] == 'TMIN'].nsmallest(10, 'temp_celsius')[['ID', 'DATE', 'temp_celsius']]\n",
    "print(coldest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precipitation Data Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to precipitation\n",
    "prcp_data = df_sample[df_sample['ELEMENT'] == 'PRCP'].copy()\n",
    "\n",
    "# Convert to mm (divide by 10, since stored in tenths of mm)\n",
    "prcp_data['precip_mm'] = prcp_data['DATA_VALUE'] / 10\n",
    "\n",
    "print(f\"Precipitation observations: {len(prcp_data):,}\")\n",
    "print(f\"\\nPrecipitation statistics (mm):\")\n",
    "print(prcp_data['precip_mm'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviest rainfall events\n",
    "print(\"Top 10 heaviest daily precipitation (mm):\")\n",
    "heaviest_rain = prcp_data.nlargest(10, 'precip_mm')[['ID', 'DATE', 'precip_mm']]\n",
    "print(heaviest_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reading Multiple Years (Efficient Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read multiple years with filtering (more efficient)\n",
    "# This reads only specific columns and filters to reduce memory\n",
    "\n",
    "def read_years_filtered(start_year, end_year, elements=['TMAX', 'TMIN', 'PRCP'], columns=None):\n",
    "    \"\"\"\n",
    "    Read multiple years of data with optional filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_year, end_year: Year range to read\n",
    "    - elements: List of elements to include (None for all)\n",
    "    - columns: List of columns to read (None for all)\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        path = f's3://{S3_BUCKET}/parquet/by_year/YEAR={year}'\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                path,\n",
    "                columns=columns,\n",
    "                storage_options={'anon': True}\n",
    "            )\n",
    "            if elements:\n",
    "                df = df[df['ELEMENT'].isin(elements)]\n",
    "            df['YEAR'] = year\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded {year}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {year}: {e}\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# Example: Read last 3 years of temperature data\n",
    "# Uncomment to run (may take a few minutes)\n",
    "# recent_temps = read_years_filtered(2021, 2023, elements=['TMAX', 'TMIN'], columns=['ID', 'DATE', 'ELEMENT', 'DATA_VALUE'])\n",
    "# print(f\"\\nTotal rows: {len(recent_temps):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Estimate Data Size for Snowflake\n",
    "\n",
    "Estimate total data volume to plan Snowflake loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file sizes for a few years\n",
    "def get_partition_size(year):\n",
    "    \"\"\"Get total size of parquet files for a year in MB.\"\"\"\n",
    "    path = f'{S3_BUCKET}/parquet/by_year/YEAR={year}'\n",
    "    try:\n",
    "        files = fs.ls(path, detail=True)\n",
    "        total_bytes = sum(f['size'] for f in files)\n",
    "        return total_bytes / (1024 * 1024)  # MB\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Sample years to estimate\n",
    "sample_years_for_size = [1900, 1950, 1980, 2000, 2010, 2020, 2023]\n",
    "print(\"Parquet file sizes by year:\")\n",
    "sizes = {}\n",
    "for year in sample_years_for_size:\n",
    "    size = get_partition_size(year)\n",
    "    sizes[year] = size\n",
    "    print(f\"  {year}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row count estimate\n",
    "print(f\"\\nRow count for sample year ({sample_year}): {len(df_sample):,}\")\n",
    "print(f\"\\nEstimated total rows (rough): ~{len(df_sample) * len(years):,}\")\n",
    "print(\"\\n(Actual total will vary as older years have fewer observations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary for Snowflake Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY FOR SNOWFLAKE LOADING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData Location: s3://noaa-ghcn-pds/parquet/by_year/\")\n",
    "print(f\"Format: Parquet (partitioned by YEAR)\")\n",
    "print(f\"Years available: {min(years)} to {max(years)} ({len(years)} years)\")\n",
    "print(f\"\\nSchema:\")\n",
    "for field in schema:\n",
    "    print(f\"  {field.name}: {field.type}\")\n",
    "print(f\"\\nSample year ({sample_year}):\")\n",
    "print(f\"  Rows: {len(df_sample):,}\")\n",
    "print(f\"  Unique stations: {df_sample['ID'].nunique():,}\")\n",
    "print(f\"  Unique elements: {df_sample['ELEMENT'].nunique()}\")\n",
    "print(f\"\\nRecommended Snowflake approach:\")\n",
    "print(\"  1. Create external stage pointing to S3 bucket\")\n",
    "print(\"  2. Use COPY INTO with Parquet format\")\n",
    "print(\"  3. Consider partitioning table by YEAR\")\n",
    "print(\"  4. Create clustering on (ELEMENT, ID) for query performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del df_sample\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
