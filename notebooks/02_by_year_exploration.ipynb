{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA GHCN By-Year Data Exploration\n",
    "\n",
    "This notebook explores the daily climate observations from the NOAA GHCN dataset, organized by year in Parquet format.\n",
    "\n",
    "**Data Source:** `s3://noaa-ghcn-pds/parquet/by_year/`\n",
    "\n",
    "**Structure:** Partitioned by year (e.g., `YEAR=2023/`)\n",
    "\n",
    "**Key Fields:**\n",
    "- `ID` - Station identifier\n",
    "- `DATE` - Observation date (YYYYMMDD)\n",
    "- `ELEMENT` - Type of observation (TMAX, TMIN, PRCP, etc.)\n",
    "- `DATA_VALUE` - The observation value\n",
    "- Quality/source flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "#pip3 install pandas pyarrow s3fs fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "# S3 filesystem (no credentials needed for public bucket)\n",
    "S3_BUCKET = 'noaa-ghcn-pds'\n",
    "PARQUET_PATH = f's3://{S3_BUCKET}/parquet/by_year'\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Available Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: 1750 to 2026\n",
      "Total years of data: 265\n",
      "\n",
      "First 10 years: [1750, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771]\n",
      "Last 10 years: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026]\n"
     ]
    }
   ],
   "source": [
    "# List all available year partitions\n",
    "year_partitions = fs.ls(f'{S3_BUCKET}/parquet/by_year/')\n",
    "years = sorted([int(p.split('YEAR=')[1].rstrip('/')) for p in year_partitions if 'YEAR=' in p])\n",
    "\n",
    "print(f\"Available years: {min(years)} to {max(years)}\")\n",
    "print(f\"Total years of data: {len(years)}\")\n",
    "print(f\"\\nFirst 10 years: {years[:10]}\")\n",
    "print(f\"Last 10 years: {years[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Schema and Sample Data\n",
    "\n",
    "Let's look at a recent year to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in YEAR=2023: 73\n",
      "\n",
      "First file: noaa-ghcn-pds/parquet/by_year/YEAR=2023/ELEMENT=ADPT\n"
     ]
    }
   ],
   "source": [
    "# Read schema from a sample file\n",
    "sample_year = 2023\n",
    "sample_path = f'{S3_BUCKET}/parquet/by_year/YEAR={sample_year}'\n",
    "\n",
    "# Get parquet files in this partition\n",
    "parquet_files = fs.ls(sample_path)\n",
    "print(f\"Files in YEAR={sample_year}: {len(parquet_files)}\")\n",
    "print(f\"\\nFirst file: {parquet_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Parquet file size is 0 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read schema\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(parquet_files[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquet Schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/parquet/core.py:2392\u001b[0m, in \u001b[0;36mread_schema\u001b[0;34m(where, memory_map, decryption_properties, filesystem)\u001b[0m\n\u001b[1;32m   2389\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m where \u001b[38;5;241m=\u001b[39m filesystem\u001b[38;5;241m.\u001b[39mopen_input_file(where)\n\u001b[1;32m   2391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx:\n\u001b[0;32m-> 2392\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mto_arrow_schema()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/parquet/core.py:328\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[0;34m(self, source, metadata, common_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification, arrow_extensions_enabled)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_memory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommon_metadata \u001b[38;5;241m=\u001b[39m common_metadata\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_paths_by_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_nested_paths()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/_parquet.pyx:1656\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Parquet file size is 0 bytes"
     ]
    }
   ],
   "source": [
    "# Read schema\n",
    "with fs.open(parquet_files[0], 'rb') as f:\n",
    "    schema = pq.read_schema(f)\n",
    "\n",
    "print(\"Parquet Schema:\")\n",
    "print(\"=\" * 40)\n",
    "for field in schema:\n",
    "    print(f\"  {field.name}: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowKeyError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read sample data from one year\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msample_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_sample\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mColumn dtypes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parquet.py:64\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parquet.py:170\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m=\u001b[39m pyarrow\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/arrays/arrow/extension_types.py:174\u001b[0m\n\u001b[1;32m    167\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[1;32m    168\u001b[0m         ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/arrays/arrow/extension_types.py:166\u001b[0m, in \u001b[0;36mpatch_pyarrow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m         pickletools\u001b[38;5;241m.\u001b[39mdis(serialized, out)\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    159\u001b[0m             _ERROR_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    160\u001b[0m                 storage_type\u001b[38;5;241m=\u001b[39mstorage_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m         )\n\u001b[0;32m--> 166\u001b[0m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow.py_extension_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[1;32m    168\u001b[0m     ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/types.pxi:2280\u001b[0m, in \u001b[0;36mpyarrow.lib.unregister_extension_type\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowKeyError\u001b[0m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "# Read sample data from one year\n",
    "df_sample = pd.read_parquet(\n",
    "    f's3://{sample_path}',\n",
    "    storage_options={'anon': True}\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(df_sample.dtypes)\n",
    "print(f\"\\nSample data:\")\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage\n",
    "print(f\"Memory usage for year {sample_year}:\")\n",
    "print(df_sample.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Element Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by element type\n",
    "element_counts = df_sample['ELEMENT'].value_counts()\n",
    "print(f\"Observation counts by element type ({sample_year}):\")\n",
    "print(element_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element descriptions\n",
    "element_descriptions = {\n",
    "    'PRCP': 'Precipitation (tenths of mm)',\n",
    "    'SNOW': 'Snowfall (mm)',\n",
    "    'SNWD': 'Snow depth (mm)',\n",
    "    'TMAX': 'Maximum temperature (tenths of degrees C)',\n",
    "    'TMIN': 'Minimum temperature (tenths of degrees C)',\n",
    "    'TAVG': 'Average temperature (tenths of degrees C)',\n",
    "    'AWND': 'Average daily wind speed (tenths of m/s)',\n",
    "    'WSFG': 'Peak gust wind speed (tenths of m/s)',\n",
    "    'WT01': 'Fog, ice fog, or freezing fog',\n",
    "    'WT02': 'Heavy fog or heavy freezing fog',\n",
    "    'WT03': 'Thunder',\n",
    "    'WT04': 'Ice pellets, sleet, snow pellets',\n",
    "    'WT05': 'Hail',\n",
    "    'WT06': 'Glaze or rime',\n",
    "    'WT07': 'Dust, volcanic ash, sand, or spray',\n",
    "    'WT08': 'Smoke or haze'\n",
    "}\n",
    "\n",
    "print(\"Common element codes and meanings:\")\n",
    "for code, desc in element_descriptions.items():\n",
    "    print(f\"  {code}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Flags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_FLAG - Measurement flag\n",
    "print(\"M_FLAG (Measurement) distribution:\")\n",
    "print(df_sample['M_FLAG'].value_counts(dropna=False).head(10))\n",
    "\n",
    "print(\"\\nM_FLAG meanings:\")\n",
    "print(\"  Blank = no flag\")\n",
    "print(\"  B = precipitation total from two 12-hour totals\")\n",
    "print(\"  D = precipitation total from four 6-hour totals\")\n",
    "print(\"  H = precipitation total from eight 3-hour totals\")\n",
    "print(\"  L = temperature lagged from previous day\")\n",
    "print(\"  T = trace of precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_FLAG - Quality flag\n",
    "print(\"Q_FLAG (Quality) distribution:\")\n",
    "print(df_sample['Q_FLAG'].value_counts(dropna=False).head(10))\n",
    "\n",
    "print(\"\\nQ_FLAG meanings:\")\n",
    "print(\"  Blank = passed all quality checks\")\n",
    "print(\"  D = failed duplicate check\")\n",
    "print(\"  G = failed gap check\")\n",
    "print(\"  I = failed internal consistency check\")\n",
    "print(\"  K = failed streak/frequent-value check\")\n",
    "print(\"  N = failed naught check\")\n",
    "print(\"  S = failed spatial consistency check\")\n",
    "print(\"  X = failed bounds check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_FLAG - Source flag\n",
    "print(\"S_FLAG (Source) distribution:\")\n",
    "print(df_sample['S_FLAG'].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Station Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations reporting in this year\n",
    "stations_in_year = df_sample['ID'].nunique()\n",
    "print(f\"Unique stations reporting in {sample_year}: {stations_in_year:,}\")\n",
    "\n",
    "# Stations by country (first 2 chars of ID)\n",
    "df_sample['country'] = df_sample['ID'].str[:2]\n",
    "stations_by_country = df_sample.groupby('country')['ID'].nunique().sort_values(ascending=False)\n",
    "print(f\"\\nTop 15 countries by station count:\")\n",
    "print(stations_by_country.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations per station\n",
    "obs_per_station = df_sample.groupby('ID').size()\n",
    "print(f\"Observations per station statistics:\")\n",
    "print(f\"  Min: {obs_per_station.min():,}\")\n",
    "print(f\"  Max: {obs_per_station.max():,}\")\n",
    "print(f\"  Mean: {obs_per_station.mean():,.0f}\")\n",
    "print(f\"  Median: {obs_per_station.median():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temperature Data Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to temperature data only\n",
    "temp_data = df_sample[df_sample['ELEMENT'].isin(['TMAX', 'TMIN', 'TAVG'])].copy()\n",
    "\n",
    "# Convert to actual Celsius (divide by 10)\n",
    "temp_data['temp_celsius'] = temp_data['DATA_VALUE'] / 10\n",
    "\n",
    "print(f\"Temperature observations: {len(temp_data):,}\")\n",
    "print(f\"\\nTemperature statistics (Celsius):\")\n",
    "temp_data.groupby('ELEMENT')['temp_celsius'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme temperatures\n",
    "print(\"Extreme TMAX values (hottest):\")\n",
    "hottest = temp_data[temp_data['ELEMENT'] == 'TMAX'].nlargest(10, 'temp_celsius')[['ID', 'DATE', 'temp_celsius']]\n",
    "print(hottest)\n",
    "\n",
    "print(\"\\nExtreme TMIN values (coldest):\")\n",
    "coldest = temp_data[temp_data['ELEMENT'] == 'TMIN'].nsmallest(10, 'temp_celsius')[['ID', 'DATE', 'temp_celsius']]\n",
    "print(coldest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precipitation Data Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to precipitation\n",
    "prcp_data = df_sample[df_sample['ELEMENT'] == 'PRCP'].copy()\n",
    "\n",
    "# Convert to mm (divide by 10, since stored in tenths of mm)\n",
    "prcp_data['precip_mm'] = prcp_data['DATA_VALUE'] / 10\n",
    "\n",
    "print(f\"Precipitation observations: {len(prcp_data):,}\")\n",
    "print(f\"\\nPrecipitation statistics (mm):\")\n",
    "print(prcp_data['precip_mm'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviest rainfall events\n",
    "print(\"Top 10 heaviest daily precipitation (mm):\")\n",
    "heaviest_rain = prcp_data.nlargest(10, 'precip_mm')[['ID', 'DATE', 'precip_mm']]\n",
    "print(heaviest_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reading Multiple Years (Efficient Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read multiple years with filtering (more efficient)\n",
    "# This reads only specific columns and filters to reduce memory\n",
    "\n",
    "def read_years_filtered(start_year, end_year, elements=['TMAX', 'TMIN', 'PRCP'], columns=None):\n",
    "    \"\"\"\n",
    "    Read multiple years of data with optional filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_year, end_year: Year range to read\n",
    "    - elements: List of elements to include (None for all)\n",
    "    - columns: List of columns to read (None for all)\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        path = f's3://{S3_BUCKET}/parquet/by_year/YEAR={year}'\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                path,\n",
    "                columns=columns,\n",
    "                storage_options={'anon': True}\n",
    "            )\n",
    "            if elements:\n",
    "                df = df[df['ELEMENT'].isin(elements)]\n",
    "            df['YEAR'] = year\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded {year}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {year}: {e}\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# Example: Read last 3 years of temperature data\n",
    "# Uncomment to run (may take a few minutes)\n",
    "# recent_temps = read_years_filtered(2021, 2023, elements=['TMAX', 'TMIN'], columns=['ID', 'DATE', 'ELEMENT', 'DATA_VALUE'])\n",
    "# print(f\"\\nTotal rows: {len(recent_temps):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Estimate Data Size for Snowflake\n",
    "\n",
    "Estimate total data volume to plan Snowflake loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file sizes for a few years\n",
    "def get_partition_size(year):\n",
    "    \"\"\"Get total size of parquet files for a year in MB.\"\"\"\n",
    "    path = f'{S3_BUCKET}/parquet/by_year/YEAR={year}'\n",
    "    try:\n",
    "        files = fs.ls(path, detail=True)\n",
    "        total_bytes = sum(f['size'] for f in files)\n",
    "        return total_bytes / (1024 * 1024)  # MB\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Sample years to estimate\n",
    "sample_years_for_size = [1900, 1950, 1980, 2000, 2010, 2020, 2023]\n",
    "print(\"Parquet file sizes by year:\")\n",
    "sizes = {}\n",
    "for year in sample_years_for_size:\n",
    "    size = get_partition_size(year)\n",
    "    sizes[year] = size\n",
    "    print(f\"  {year}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row count estimate\n",
    "print(f\"\\nRow count for sample year ({sample_year}): {len(df_sample):,}\")\n",
    "print(f\"\\nEstimated total rows (rough): ~{len(df_sample) * len(years):,}\")\n",
    "print(\"\\n(Actual total will vary as older years have fewer observations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary for Snowflake Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY FOR SNOWFLAKE LOADING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData Location: s3://noaa-ghcn-pds/parquet/by_year/\")\n",
    "print(f\"Format: Parquet (partitioned by YEAR)\")\n",
    "print(f\"Years available: {min(years)} to {max(years)} ({len(years)} years)\")\n",
    "print(f\"\\nSchema:\")\n",
    "for field in schema:\n",
    "    print(f\"  {field.name}: {field.type}\")\n",
    "print(f\"\\nSample year ({sample_year}):\")\n",
    "print(f\"  Rows: {len(df_sample):,}\")\n",
    "print(f\"  Unique stations: {df_sample['ID'].nunique():,}\")\n",
    "print(f\"  Unique elements: {df_sample['ELEMENT'].nunique()}\")\n",
    "print(f\"\\nRecommended Snowflake approach:\")\n",
    "print(\"  1. Create external stage pointing to S3 bucket\")\n",
    "print(\"  2. Use COPY INTO with Parquet format\")\n",
    "print(\"  3. Consider partitioning table by YEAR\")\n",
    "print(\"  4. Create clustering on (ELEMENT, ID) for query performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del df_sample\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
